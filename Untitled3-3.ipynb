{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTShjSXe8wHY",
        "outputId": "8913f7aa-4e41-4a8f-9fe1-99dbe3622d29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.8-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting meteostat\n",
            "  Downloading meteostat-1.7.6-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting xgboost\n",
            "  Downloading xgboost-3.1.2-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting lightgbm\n",
            "  Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
            "Collecting prophet\n",
            "  Downloading prophet-1.2.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting statsmodels\n",
            "  Downloading statsmodels-0.14.6-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting pywavelets\n",
            "  Downloading pywavelets-1.9.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
            "Collecting graphviz (from catboost)\n",
            "  Downloading graphviz-0.21-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (3.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (2.32.4)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (1.76.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (6.33.1)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.17.2-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting sqlalchemy>=1.4.2 (from optuna)\n",
            "  Downloading sqlalchemy-2.0.44-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from meteostat) (2025.2)\n",
            "Collecting nvidia-nccl-cu12 (from xgboost)\n",
            "  Downloading nvidia_nccl_cu12-2.28.9-py3-none-manylinux_2_18_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting cmdstanpy>=1.0.4 (from prophet)\n",
            "  Downloading cmdstanpy-1.3.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting holidays<1,>=0.25 (from prophet)\n",
            "  Downloading holidays-0.86-py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from prophet) (6.5.2)\n",
            "Collecting patsy>=0.5.6 (from statsmodels)\n",
            "  Downloading patsy-1.0.2-py2.py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting stanio<2.0.0,>=0.4.0 (from cmdstanpy>=1.0.4->prophet)\n",
            "  Downloading stanio-0.5.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from holidays<1,>=0.25->prophet) (2.9.0.post0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (14.2.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (0.18.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (12.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->keras-tuner) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->keras-tuner) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->keras-tuner) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->keras-tuner) (2025.11.12)\n",
            "Collecting greenlet>=1 (from sqlalchemy>=1.4.2->optuna)\n",
            "  Downloading greenlet-3.3.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.3.6)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.4-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras->keras-tuner) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras->keras-tuner) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras_tuner-1.4.8-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading meteostat-1.7.6-py3-none-any.whl (33 kB)\n",
            "Downloading xgboost-3.1.2-py3-none-manylinux_2_28_x86_64.whl (115.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.9/115.9 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m148.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prophet-1.2.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading statsmodels-0.14.6-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m184.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pywavelets-1.9.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m173.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.17.2-py3-none-any.whl (248 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m248.6/248.6 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading cmdstanpy-1.3.0-py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading holidays-0.86-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m149.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading patsy-1.0.2-py2.py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m233.3/233.3 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sqlalchemy-2.0.44-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m163.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m164.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Downloading graphviz-0.21-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Downloading nvidia_nccl_cu12-2.28.9-py3-none-manylinux_2_18_x86_64.whl (296.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m296.8/296.8 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading greenlet-3.3.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (609 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m609.9/609.9 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stanio-0.5.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.4-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m225.0/225.0 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, kt-legacy, flatbuffers, wheel, werkzeug, tensorboard-data-server, stanio, pywavelets, patsy, nvidia-nccl-cu12, greenlet, graphviz, google_pasta, colorlog, xgboost, tensorboard, sqlalchemy, lightgbm, holidays, astunparse, statsmodels, meteostat, cmdstanpy, catboost, alembic, tensorflow, prophet, optuna, keras-tuner\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CELL 2: IMPORTS & GLOBAL CONFIG\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\"\n",
        "Import all libraries and set global configurations\n",
        "\"\"\"\n",
        "\n",
        "!pip install catboost keras-tuner optuna meteostat xgboost lightgbm prophet statsmodels tensorflow pywavelets\n",
        "\n",
        "# Core Libraries\n",
        "import os\n",
        "import gc\n",
        "import sys\n",
        "import json\n",
        "import pickle\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict\n",
        "# Data Processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "# ML - Preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "# ML - Traditional Models\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
        "# ML - Time Series Specific\n",
        "from prophet import Prophet\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "# ML - Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, callbacks, optimizers\n",
        "from tensorflow.keras.layers import LSTM, GRU, Bidirectional, Conv1D, Dense, Dropout, BatchNormalization\n",
        "from keras_tuner import RandomSearch, Hyperband\n",
        "# ML - Hyperparameter Tuning\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "# External Data\n",
        "import holidays\n",
        "from meteostat import Hourly, Point\n",
        "# Google Drive\n",
        "from google.colab import drive\n",
        "# Configurations\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "# TensorFlow Configurations\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "# Global Constants\n",
        "NYC_COORDS = Point(40.7128, -74.0060)\n",
        "TOP_N_STATIONS = 500\n",
        "SEQ_LEN = 24\n",
        "FORECAST_HORIZON = 48\n",
        "TRAIN_TEST_SPLIT = 0.8\n",
        "print(\"âœ… All imports successful!\")\n",
        "print(f\"ğŸ“¦ TensorFlow version: {tf.__version__}\")\n",
        "print(f\"ğŸ“¦ XGBoost version: {xgb.__version__}\")\n",
        "print(f\"ğŸ“¦ LightGBM version: {lgb.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYgbGABL8zbB"
      },
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CELL 3: SETUP DIRECTORIES (SAVE TO DRIVE - CRASH PROOF!)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "import os\n",
        "\n",
        "# âš ï¸ CRITICAL: Save to Drive, not /content/ (which gets wiped on disconnect)\n",
        "BASE_DIR = \"/content/drive/MyDrive/citi_bike_project\"\n",
        "CHECKPOINT_DIR = f\"{BASE_DIR}/checkpoints\"\n",
        "OUTPUT_DIR = f\"{BASE_DIR}/final_output\"\n",
        "PLOTS_DIR = f\"{BASE_DIR}/plots\"\n",
        "\n",
        "# Create directories\n",
        "for directory in [CHECKPOINT_DIR, OUTPUT_DIR, PLOTS_DIR]:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "print(f\"âœ… Directories created in Google Drive\")\n",
        "print(f\"ğŸ“ Checkpoints: {CHECKPOINT_DIR}\")\n",
        "print(f\"ğŸ“ Models: {OUTPUT_DIR}\")\n",
        "print(f\"ğŸ“ Plots: {PLOTS_DIR}\")\n",
        "print(f\"\\nâš¡ All files will persist even if Colab disconnects!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTim_d8R87r8"
      },
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CELL 4: LOAD & INSPECT RAW DATA\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\"\n",
        "Load raw data and perform initial inspection\n",
        "\"\"\"\n",
        "print(\"ğŸ“Š Loading raw data...\")\n",
        "df_raw = pd.read_parquet(RAW_DATA_PATH)\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"ğŸ“‹ RAW DATA OVERVIEW\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Shape: {df_raw.shape}\")\n",
        "print(f\"Memory Usage: {df_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "print(f\"\\nDate Range: {df_raw['started_at'].min()} to {df_raw['started_at'].max()}\")\n",
        "print(f\"Total Days: {(df_raw['started_at'].max() - df_raw['started_at'].min()).days}\")\n",
        "print(f\"\\nColumns: {list(df_raw.columns)}\")\n",
        "print(f\"\\nData Types:\\n{df_raw.dtypes}\")\n",
        "print(f\"\\nMissing Values:\\n{df_raw.isnull().sum()}\")\n",
        "# Quick statistics\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"ğŸ“ˆ QUICK STATISTICS\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Total Trips: {len(df_raw):,}\")\n",
        "print(f\"Unique Start Stations: {df_raw['start_station_name'].nunique():,}\")\n",
        "print(f\"Unique End Stations: {df_raw['end_station_name'].nunique():,}\")\n",
        "# Save checkpoint\n",
        "df_raw.to_parquet(f\"{CHECKPOINT_DIR}/01_raw_data.parquet\", index=False)\n",
        "print(f\"\\nâœ… Checkpoint saved: 01_raw_data.parquet\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFFRdOeD9TrT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CELL 5: DATA AGGREGATION - HOURLY NET DEMAND\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\"\n",
        "Aggregate trip data to hourly net demand per station\n",
        "Net Demand = Arrivals - Departures\n",
        "\"\"\"\n",
        "print(\"ğŸ”„ Aggregating trips to hourly net demand...\")\n",
        "# Ensure datetime types\n",
        "df_raw['started_at'] = pd.to_datetime(df_raw['started_at'])\n",
        "df_raw['ended_at'] = pd.to_datetime(df_raw['ended_at'])\n",
        "# Floor to hourly bins\n",
        "df_raw['start_h'] = df_raw['started_at'].dt.floor('h')\n",
        "df_raw['end_h'] = df_raw['ended_at'].dt.floor('h')\n",
        "# Outbound trips (departures = negative demand)\n",
        "outbound = df_raw.groupby(['start_station_name', 'start_h']).size().reset_index(name='count')\n",
        "outbound.columns = ['station_name', 'time', 'count']\n",
        "outbound['net_demand'] = -outbound['count']\n",
        "# Inbound trips (arrivals = positive demand)\n",
        "inbound = df_raw.groupby(['end_station_name', 'end_h']).size().reset_index(name='count')\n",
        "inbound.columns = ['station_name', 'time', 'count']\n",
        "inbound['net_demand'] = inbound['count']\n",
        "# Combine and aggregate\n",
        "df_agg = pd.concat([\n",
        "    outbound[['station_name', 'time', 'net_demand']],\n",
        "    inbound[['station_name', 'time', 'net_demand']]\n",
        "], ignore_index=True)\n",
        "df_agg = df_agg.groupby(['station_name', 'time'])['net_demand'].sum().reset_index()\n",
        "print(f\"\\nâœ… Aggregation complete!\")\n",
        "print(f\"   Shape: {df_agg.shape}\")\n",
        "print(f\"   Date Range: {df_agg['time'].min()} to {df_agg['time'].max()}\")\n",
        "print(f\"   Stations: {df_agg['station_name'].nunique()}\")\n",
        "# Clean up memory\n",
        "del df_raw, outbound, inbound\n",
        "gc.collect()\n",
        "# Save checkpoint\n",
        "df_agg.to_parquet(f\"{CHECKPOINT_DIR}/02_aggregated_demand.parquet\", index=False)\n",
        "print(f\"âœ… Checkpoint saved: 02_aggregated_demand.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HS6rpC9A9ZVU"
      },
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CELL 6: SELECT TOP 500 HIGH-QUALITY STATIONS (OPTIMIZED)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\"\n",
        "Select top 500 stations using vectorized operations (FAST!)\n",
        "\"\"\"\n",
        "\n",
        "print(\"ğŸ¯ Selecting top 500 high-quality stations (optimized)...\")\n",
        "\n",
        "# Use vectorized operations instead of loops\n",
        "station_stats = df_agg.groupby('station_name').agg({\n",
        "    'net_demand': ['count', lambda x: x.abs().sum(), lambda x: x.abs().mean(), lambda x: x.abs().std()],\n",
        "    'time': ['min', 'max']\n",
        "}).reset_index()\n",
        "\n",
        "station_stats.columns = ['station_name', 'data_points', 'total_volume', 'mean_demand', 'std_demand', 'first_time', 'last_time']\n",
        "\n",
        "# Calculate additional metrics\n",
        "station_stats['cv'] = station_stats['std_demand'] / (station_stats['mean_demand'] + 1e-6)\n",
        "station_stats['days_active'] = (station_stats['last_time'] - station_stats['first_time']).dt.total_seconds() / 86400\n",
        "station_stats['coverage_ratio'] = station_stats['data_points'] / (station_stats['days_active'] * 24 + 1)\n",
        "station_stats['coverage_ratio'] = station_stats['coverage_ratio'].clip(0, 1)\n",
        "\n",
        "# Quality score: high volume, good coverage, low noise\n",
        "station_stats['quality_score'] = (\n",
        "    station_stats['total_volume'] *\n",
        "    station_stats['coverage_ratio'] /\n",
        "    (station_stats['cv'] + 1)\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… Metrics calculated for {len(station_stats)} stations!\")\n",
        "print(f\"\\nTop 10 by quality score:\")\n",
        "print(station_stats.nlargest(10, 'quality_score')[['station_name', 'total_volume', 'coverage_ratio', 'cv', 'quality_score']])\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "ax1 = axes[0, 0]\n",
        "station_stats.nlargest(20, 'total_volume')[['station_name', 'total_volume']].set_index('station_name').plot(kind='barh', ax=ax1, legend=False)\n",
        "ax1.set_title('Top 20 Stations by Total Volume')\n",
        "ax1.set_xlabel('Total Volume')\n",
        "\n",
        "ax2 = axes[0, 1]\n",
        "station_stats['coverage_ratio'].hist(bins=50, ax=ax2, edgecolor='black')\n",
        "ax2.set_title('Data Coverage Distribution')\n",
        "ax2.set_xlabel('Coverage Ratio')\n",
        "ax2.axvline(0.9, color='red', linestyle='--', label='90% threshold')\n",
        "ax2.legend()\n",
        "\n",
        "ax3 = axes[1, 0]\n",
        "station_stats['cv'].clip(0, 5).hist(bins=50, ax=ax3, edgecolor='black')\n",
        "ax3.set_title('Coefficient of Variation Distribution')\n",
        "ax3.set_xlabel('CV (capped at 5)')\n",
        "ax3.axvline(2.0, color='red', linestyle='--', label='High noise')\n",
        "ax3.legend()\n",
        "\n",
        "ax4 = axes[1, 1]\n",
        "scatter = ax4.scatter(\n",
        "    station_stats['total_volume'],\n",
        "    station_stats['coverage_ratio'],\n",
        "    c=station_stats['cv'].clip(0, 3),\n",
        "    cmap='RdYlGn_r',\n",
        "    alpha=0.6,\n",
        "    s=50\n",
        ")\n",
        "ax4.set_xlabel('Total Volume')\n",
        "ax4.set_ylabel('Coverage Ratio')\n",
        "ax4.set_title('Volume vs Coverage (color = CV)')\n",
        "ax4.set_xscale('log')\n",
        "plt.colorbar(scatter, ax=ax4, label='CV')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{PLOTS_DIR}/01_station_selection_metrics.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Select top 500\n",
        "top_stations = station_stats.nlargest(TOP_N_STATIONS, 'quality_score')['station_name'].tolist()\n",
        "\n",
        "print(f\"\\nâœ… Selected {len(top_stations)} stations\")\n",
        "print(f\"\\nğŸ“Š Selected Stations Statistics:\")\n",
        "print(station_stats[station_stats['station_name'].isin(top_stations)].describe())\n",
        "\n",
        "# Filter dataset (FAST with isin)\n",
        "df_filtered = df_agg[df_agg['station_name'].isin(top_stations)].copy()\n",
        "print(f\"\\nâœ… Filtered dataset shape: {df_filtered.shape}\")\n",
        "\n",
        "# Save checkpoint\n",
        "df_filtered.to_parquet(f\"{CHECKPOINT_DIR}/03_top500_filtered.parquet\", index=False)\n",
        "with open(f\"{CHECKPOINT_DIR}/03_top500_stations.json\", 'w') as f:\n",
        "    json.dump(top_stations, f, indent=2)\n",
        "print(f\"âœ… Checkpoint saved: 03_top500_filtered.parquet\")\n",
        "\n",
        "# Clean up\n",
        "del df_agg, station_stats\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXoTzcmn9b1J"
      },
      "outputs": [],
      "source": [
        "# CELL 7: CREATE PERFECT TIME GRID (FILL MISSING HOURS)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\"\n",
        "Create a complete hourly grid for all stations\n",
        "Fill missing hours with 0 demand (no trips)\n",
        "\"\"\"\n",
        "print(\"ğŸ• Creating perfect hourly grid...\")\n",
        "# Get time range\n",
        "min_time = df_filtered['time'].min()\n",
        "max_time = df_filtered['time'].max()\n",
        "print(f\"Time range: {min_time} to {max_time}\")\n",
        "print(f\"Total hours: {(max_time - min_time).total_seconds() / 3600:.0f}\")\n",
        "# Create complete time grid\n",
        "time_grid = pd.date_range(start=min_time, end=max_time, freq='h')\n",
        "# Create product of stations x times\n",
        "from itertools import product\n",
        "grid = pd.DataFrame(\n",
        "    list(product(top_stations, time_grid)),\n",
        "    columns=['station_name', 'time']\n",
        ")\n",
        "print(f\"Grid shape: {grid.shape}\")\n",
        "# Merge with actual data\n",
        "df_complete = grid.merge(df_filtered, on=['station_name', 'time'], how='left')\n",
        "df_complete['net_demand'] = df_complete['net_demand'].fillna(0)\n",
        "print(f\"\\nâœ… Complete grid created!\")\n",
        "print(f\"   Shape: {df_complete.shape}\")\n",
        "print(f\"   Missing hours filled: {df_complete['net_demand'].isna().sum()}\")\n",
        "# Save checkpoint\n",
        "df_complete.to_parquet(f\"{CHECKPOINT_DIR}/04_complete_grid.parquet\", index=False)\n",
        "print(f\"âœ… Checkpoint saved: 04_complete_grid.parquet\")\n",
        "del df_filtered, grid\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5KTGS1l9n0L"
      },
      "outputs": [],
      "source": [
        "# CELL 8: FETCH WEATHER DATA (NYC HOURLY 2020-2025)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\"\n",
        "Fetch historical weather data from Meteostat\n",
        "Temperature, Precipitation, Wind Speed\n",
        "\"\"\"\n",
        "print(\"ğŸŒ¤ï¸  Fetching weather data from Meteostat...\")\n",
        "try:\n",
        "    # Fetch weather in chunks to avoid timeouts\n",
        "    weather_data = []\n",
        "    start = df_complete['time'].min()\n",
        "    end = df_complete['time'].max()\n",
        "\n",
        "    # Split into yearly chunks\n",
        "    current_start = start\n",
        "    while current_start < end:\n",
        "        current_end = min(current_start + pd.DateOffset(months=6), end)\n",
        "\n",
        "        print(f\"   Fetching {current_start.date()} to {current_end.date()}...\")\n",
        "\n",
        "        weather_chunk = Hourly(NYC_COORDS, current_start, current_end)\n",
        "        weather_df = weather_chunk.fetch()\n",
        "\n",
        "        if not weather_df.empty:\n",
        "            weather_data.append(weather_df)\n",
        "\n",
        "        current_start = current_end + pd.Timedelta(hours=1)\n",
        "\n",
        "    # Combine all chunks\n",
        "    weather_combined = pd.concat(weather_data, ignore_index=False)\n",
        "    weather_combined = weather_combined.reset_index()\n",
        "    weather_combined = weather_combined.rename(columns={'time': 'time'})\n",
        "\n",
        "    # Select relevant columns\n",
        "    weather_combined = weather_combined[['time', 'temp', 'prcp', 'wspd']].copy()\n",
        "\n",
        "    # Forward fill missing values\n",
        "    weather_combined = weather_combined.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "    # Fill any remaining NaN with reasonable defaults\n",
        "    weather_combined['temp'] = weather_combined['temp'].fillna(15.0)\n",
        "    weather_combined['prcp'] = weather_combined['prcp'].fillna(0.0)\n",
        "    weather_combined['wspd'] = weather_combined['wspd'].fillna(5.0)\n",
        "\n",
        "    print(f\"\\nâœ… Weather data fetched!\")\n",
        "    print(f\"   Shape: {weather_combined.shape}\")\n",
        "    print(f\"   Date range: {weather_combined['time'].min()} to {weather_combined['time'].max()}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸  Error fetching weather: {e}\")\n",
        "    print(\"Using synthetic weather data...\")\n",
        "\n",
        "    # Fallback: Create synthetic weather with seasonal patterns\n",
        "    unique_times = df_complete['time'].unique()\n",
        "    weather_combined = pd.DataFrame({'time': unique_times})\n",
        "\n",
        "    # Temperature: seasonal sine wave + daily cycle\n",
        "    days_since_start = (weather_combined['time'] - weather_combined['time'].min()).dt.total_seconds() / 86400\n",
        "    weather_combined['temp'] = (\n",
        "        15 + 15 * np.sin(2 * np.pi * days_since_start / 365) +  # Seasonal\n",
        "        5 * np.sin(2 * np.pi * weather_combined['time'].dt.hour / 24)  # Daily\n",
        "    )\n",
        "\n",
        "    # Precipitation: random with some clustering\n",
        "    weather_combined['prcp'] = np.random.exponential(0.5, len(weather_combined))\n",
        "    weather_combined.loc[weather_combined['prcp'] > 5, 'prcp'] = 0  # Cap outliers\n",
        "\n",
        "    # Wind speed: random with seasonal bias\n",
        "    weather_combined['wspd'] = 10 + 5 * np.sin(2 * np.pi * days_since_start / 365) + np.random.normal(0, 2, len(weather_combined))\n",
        "    weather_combined['wspd'] = weather_combined['wspd'].clip(0, 30)\n",
        "# Merge weather into main dataset\n",
        "df_complete = df_complete.merge(weather_combined, on='time', how='left')\n",
        "# Fill any remaining NaN\n",
        "df_complete['temp'] = df_complete['temp'].fillna(method='ffill').fillna(15.0)\n",
        "df_complete['prcp'] = df_complete['prcp'].fillna(0.0)\n",
        "df_complete['wspd'] = df_complete['wspd'].fillna(5.0)\n",
        "print(f\"\\nâœ… Weather merged into dataset!\")\n",
        "print(f\"   Final shape: {df_complete.shape}\")\n",
        "# Save checkpoint\n",
        "df_complete.to_parquet(f\"{CHECKPOINT_DIR}/05_with_weather.parquet\", index=False)\n",
        "print(f\"âœ… Checkpoint saved: 05_with_weather.parquet\")\n",
        "del weather_combined\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQDUWeQT_FG-"
      },
      "source": [
        "#phase2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNfQCfZ790Z5"
      },
      "outputs": [],
      "source": [
        "# CELL 9: HOLIDAY FEATURES\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\"\n",
        "Add comprehensive holiday features\n",
        "- US Federal holidays\n",
        "- NYC-specific holidays\n",
        "- Holiday proximity features\n",
        "- Weekend features\n",
        "\"\"\"\n",
        "print(\"ğŸ‰ Adding holiday features...\")\n",
        "# US Holidays (Federal + State)\n",
        "us_holidays = holidays.US(state='NY', years=range(2020, 2026))\n",
        "# Add holiday indicators\n",
        "df_complete['is_holiday'] = df_complete['time'].apply(lambda x: 1 if x.date() in us_holidays else 0).astype('int8')\n",
        "# Day of week features\n",
        "df_complete['day_of_week'] = df_complete['time'].dt.dayofweek\n",
        "df_complete['is_weekend'] = (df_complete['day_of_week'] >= 5).astype('int8')\n",
        "# Holiday proximity (days until next holiday, days since last holiday)\n",
        "def days_to_next_holiday(date):\n",
        "    future_holidays = [h for h in us_holidays.keys() if h > date]\n",
        "    if future_holidays:\n",
        "        return (min(future_holidays) - date).days\n",
        "    return 999\n",
        "def days_since_last_holiday(date):\n",
        "    past_holidays = [h for h in us_holidays.keys() if h < date]\n",
        "    if past_holidays:\n",
        "        return (date - max(past_holidays)).days\n",
        "    return 999\n",
        "# Sample for efficiency (exact calculation takes too long)\n",
        "sample_dates = df_complete['time'].dt.date.unique()\n",
        "holiday_proximity = {}\n",
        "for date in sample_dates:\n",
        "    holiday_proximity[date] = {\n",
        "        'days_to_holiday': days_to_next_holiday(date),\n",
        "        'days_since_holiday': days_since_last_holiday(date)\n",
        "    }\n",
        "df_complete['days_to_holiday'] = df_complete['time'].dt.date.map(lambda x: holiday_proximity[x]['days_to_holiday']).astype('int16')\n",
        "df_complete['days_since_holiday'] = df_complete['time'].dt.date.map(lambda x: holiday_proximity[x]['days_since_holiday']).astype('int16')\n",
        "print(f\"âœ… Holiday features added!\")\n",
        "print(f\"   Total holidays in dataset: {df_complete['is_holiday'].sum()}\")\n",
        "print(f\"   Weekend hours: {df_complete['is_weekend'].sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPG1eOFC_UAX"
      },
      "outputs": [],
      "source": [
        "# CELL 10: ADVANCED TEMPORAL FEATURES\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\"\n",
        "Create 40+ temporal and cyclical features\n",
        "\"\"\"\n",
        "print(\"â° Engineering advanced temporal features...\")\n",
        "# Basic time components\n",
        "df_complete['hour'] = df_complete['time'].dt.hour\n",
        "df_complete['day'] = df_complete['time'].dt.day\n",
        "df_complete['month'] = df_complete['time'].dt.month\n",
        "df_complete['year'] = df_complete['time'].dt.year\n",
        "df_complete['quarter'] = df_complete['time'].dt.quarter\n",
        "# Cyclical encoding (sine/cosine transformations)\n",
        "df_complete['hour_sin'] = np.sin(2 * np.pi * df_complete['hour'] / 24)\n",
        "df_complete['hour_cos'] = np.cos(2 * np.pi * df_complete['hour'] / 24)\n",
        "df_complete['month_sin'] = np.sin(2 * np.pi * df_complete['month'] / 12)\n",
        "df_complete['month_cos'] = np.cos(2 * np.pi * df_complete['month'] / 12)\n",
        "df_complete['day_sin'] = np.sin(2 * np.pi * df_complete['day'] / 31)\n",
        "df_complete['day_cos'] = np.cos(2 * np.pi * df_complete['day'] / 31)\n",
        "# Rush hour indicators\n",
        "df_complete['is_morning_rush'] = ((df_complete['hour'] >= 7) & (df_complete['hour'] <= 9)).astype('int8')\n",
        "df_complete['is_evening_rush'] = ((df_complete['hour'] >= 17) & (df_complete['hour'] <= 19)).astype('int8')\n",
        "df_complete['is_rush_hour'] = (df_complete['is_morning_rush'] | df_complete['is_evening_rush']).astype('int8')\n",
        "# Business hours\n",
        "df_complete['is_business_hours'] = ((df_complete['hour'] >= 9) & (df_complete['hour'] <= 17) & (df_complete['day_of_week'] < 5)).astype('int8')\n",
        "# Time of day categories\n",
        "def categorize_time_of_day(hour):\n",
        "    if 5 <= hour < 12: return 1  # Morning\n",
        "    elif 12 <= hour < 17: return 2  # Afternoon\n",
        "    elif 17 <= hour < 21: return 3  # Evening\n",
        "    else: return 4  # Night\n",
        "df_complete['time_of_day'] = df_complete['hour'].apply(categorize_time_of_day).astype('int8')\n",
        "# Season\n",
        "def get_season(month):\n",
        "    if month in [12, 1, 2]: return 1  # Winter\n",
        "    elif month in [3, 4, 5]: return 2  # Spring\n",
        "    elif month in [6, 7, 8]: return 3  # Summer\n",
        "    else: return 4  # Fall\n",
        "df_complete['season'] = df_complete['month'].apply(get_season).astype('int8')\n",
        "# Week of year\n",
        "df_complete['week_of_year'] = df_complete['time'].dt.isocalendar().week.astype('int8')\n",
        "# Special periods\n",
        "df_complete['is_month_start'] = (df_complete['day'] <= 3).astype('int8')\n",
        "df_complete['is_month_end'] = (df_complete['day'] >= 28).astype('int8')\n",
        "# Days since epoch (for trend)\n",
        "epoch = pd.Timestamp('2020-01-01')\n",
        "df_complete['days_since_epoch'] = (df_complete['time'] - epoch).dt.total_seconds() / 86400\n",
        "print(f\"âœ… Temporal features created!\")\n",
        "print(f\"   New columns: {len([c for c in df_complete.columns if c not in ['station_name', 'time', 'net_demand']])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNXxxdMs_VAp"
      },
      "outputs": [],
      "source": [
        "# CELL 11: LAG & ROLLING WINDOW FEATURES (PER STATION)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\"\n",
        "Create powerful lag and rolling window features\n",
        "This is memory-intensive but critical for performance\n",
        "\"\"\"\n",
        "print(\"ğŸ”„ Creating lag and rolling window features (this may take a few minutes)...\")\n",
        "# Sort by station and time\n",
        "df_complete = df_complete.sort_values(['station_name', 'time']).reset_index(drop=True)\n",
        "# Group by station\n",
        "grouped = df_complete.groupby('station_name')['net_demand']\n",
        "# Lag features (1h, 2h, 3h, 6h, 12h, 24h, 48h, 168h=1week)\n",
        "lags = [1, 2, 3, 6, 12, 24, 48, 168]\n",
        "for lag in lags:\n",
        "    print(f\"   Creating lag_{lag}h...\")\n",
        "    df_complete[f'lag_{lag}h'] = grouped.shift(lag).astype('float32')\n",
        "# Rolling window statistics (last 4h, 12h, 24h, 168h)\n",
        "windows = [4, 12, 24, 168]\n",
        "for window in windows:\n",
        "    print(f\"   Creating rolling window {window}h statistics...\")\n",
        "\n",
        "    # Mean\n",
        "    df_complete[f'roll_mean_{window}h'] = grouped.transform(\n",
        "        lambda x: x.shift(1).rolling(window, min_periods=1).mean()\n",
        "    ).astype('float32')\n",
        "\n",
        "    # Std\n",
        "    df_complete[f'roll_std_{window}h'] = grouped.transform(\n",
        "        lambda x: x.shift(1).rolling(window, min_periods=1).std()\n",
        "    ).astype('float32')\n",
        "\n",
        "    # Min\n",
        "    df_complete[f'roll_min_{window}h'] = grouped.transform(\n",
        "        lambda x: x.shift(1).rolling(window, min_periods=1).min()\n",
        "    ).astype('float32')\n",
        "\n",
        "    # Max\n",
        "    df_complete[f'roll_max_{window}h'] = grouped.transform(\n",
        "        lambda x: x.shift(1).rolling(window, min_periods=1).max()\n",
        "    ).astype('float32')\n",
        "# Exponential moving average\n",
        "df_complete['ema_24h'] = grouped.transform(\n",
        "    lambda x: x.shift(1).ewm(span=24, adjust=False).mean()\n",
        ").astype('float32')\n",
        "# Rate of change features\n",
        "df_complete['demand_change_1h'] = (df_complete['net_demand'] - df_complete['lag_1h']).astype('float32')\n",
        "df_complete['demand_change_24h'] = (df_complete['net_demand'] - df_complete['lag_24h']).astype('float32')\n",
        "print(f\"âœ… Lag and rolling features created!\")\n",
        "print(f\"   Total features now: {df_complete.shape[1]}\")\n",
        "# Save checkpoint\n",
        "df_complete.to_parquet(f\"{CHECKPOINT_DIR}/06_with_lag_features.parquet\", index=False)\n",
        "print(f\"âœ… Checkpoint saved: 06_with_lag_features.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8oFrh0l_cpO"
      },
      "outputs": [],
      "source": [
        "# CELL 12: FOURIER TRANSFORM FEATURES (DETECT PERIODIC PATTERNS)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\"\n",
        "Use FFT to capture periodic patterns in demand\n",
        "\"\"\"\n",
        "print(\"ğŸ“Š Applying Fourier Transform for periodic pattern detection...\")\n",
        "from scipy.fft import fft\n",
        "# For each station, compute FFT features\n",
        "fourier_features = []\n",
        "for station in top_stations[:50]:  # Sample to save computation time\n",
        "    station_data = df_complete[df_complete['station_name'] == station]['net_demand'].values\n",
        "\n",
        "    if len(station_data) > 168:  # At least 1 week of data\n",
        "        # Compute FFT\n",
        "        fft_vals = fft(station_data)\n",
        "        power = np.abs(fft_vals) ** 2\n",
        "\n",
        "        # Get dominant frequencies\n",
        "        freqs = np.fft.fftfreq(len(station_data))\n",
        "\n",
        "        # Find top 3 frequencies (excluding DC component)\n",
        "        top_freq_idx = np.argsort(power[1:])[-3:] + 1\n",
        "\n",
        "        fourier_features.append({\n",
        "            'station_name': station,\n",
        "            'dominant_freq_1': freqs[top_freq_idx[0]],\n",
        "            'dominant_freq_2': freqs[top_freq_idx[1]],\n",
        "            'dominant_freq_3': freqs[top_freq_idx[2]],\n",
        "            'power_1': power[top_freq_idx[0]],\n",
        "            'power_2': power[top_freq_idx[1]],\n",
        "            'power_3': power[top_freq_idx[2]]\n",
        "        })\n",
        "df_fourier = pd.DataFrame(fourier_features)\n",
        "df_complete = df_complete.merge(df_fourier, on='station_name', how='left')\n",
        "# Fill NaN for stations not in sample\n",
        "fourier_cols = [c for c in df_fourier.columns if c != 'station_name']\n",
        "for col in fourier_cols:\n",
        "    df_complete[col] = df_complete[col].fillna(df_complete[col].median()).astype('float32')\n",
        "print(f\"âœ… Fourier features added!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaJe4VqC_i9X"
      },
      "outputs": [],
      "source": [
        "# CELL 13: WEATHER INTERACTION FEATURES\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\"\n",
        "Create interaction features between weather and time periods\n",
        "\"\"\"\n",
        "print(\"â˜ï¸ Creating weather interaction features...\")\n",
        "# Temperature interactions\n",
        "df_complete['temp_hour_interaction'] = df_complete['temp'] * df_complete['hour'] / 24\n",
        "df_complete['temp_weekend_interaction'] = df_complete['temp'] * df_complete['is_weekend']\n",
        "df_complete['temp_rush_interaction'] = df_complete['temp'] * df_complete['is_rush_hour']\n",
        "# Precipitation impact\n",
        "df_complete['rain_weekend'] = df_complete['prcp'] * df_complete['is_weekend']\n",
        "df_complete['rain_rush_hour'] = df_complete['prcp'] * df_complete['is_rush_hour']\n",
        "# Comfort index (feels-like temperature considering wind)\n",
        "df_complete['feels_like'] = df_complete['temp'] - 0.4 * df_complete['wspd']\n",
        "# Weather extremes\n",
        "df_complete['is_hot'] = (df_complete['temp'] > df_complete['temp'].quantile(0.9)).astype('int8')\n",
        "df_complete['is_cold'] = (df_complete['temp'] < df_complete['temp'].quantile(0.1)).astype('int8')\n",
        "df_complete['is_rainy'] = (df_complete['prcp'] > 1.0).astype('int8')\n",
        "df_complete['is_windy'] = (df_complete['wspd'] > df_complete['wspd'].quantile(0.9)).astype('int8')\n",
        "# Bad weather indicator\n",
        "df_complete['bad_weather'] = (df_complete['is_rainy'] | df_complete['is_cold'] | df_complete['is_windy']).astype('int8')\n",
        "print(f\"âœ… Weather interaction features created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWPApoPE_our"
      },
      "outputs": [],
      "source": [
        "# CELL 14: COMPREHENSIVE EDA & VISUALIZATION\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\"\n",
        "Visualize patterns before outlier detection\n",
        "\"\"\"\n",
        "print(\"ğŸ“Š Generating comprehensive EDA visualizations...\")\n",
        "# Save original for comparison\n",
        "df_before_outliers = df_complete.copy()\n",
        "# 1. Demand distribution\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "ax1 = axes[0, 0]\n",
        "df_complete['net_demand'].hist(bins=100, ax=ax1, edgecolor='black')\n",
        "ax1.set_title('Net Demand Distribution (All Data)')\n",
        "ax1.set_xlabel('Net Demand')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.axvline(0, color='red', linestyle='--', label='Zero')\n",
        "ax1.legend()\n",
        "ax2 = axes[0, 1]\n",
        "# Hourly patterns\n",
        "hourly_avg = df_complete.groupby('hour')['net_demand'].mean()\n",
        "hourly_avg.plot(kind='bar', ax=ax2, color='steelblue')\n",
        "ax2.set_title('Average Demand by Hour of Day')\n",
        "ax2.set_xlabel('Hour')\n",
        "ax2.set_ylabel('Average Net Demand')\n",
        "ax3 = axes[1, 0]\n",
        "# Day of week patterns\n",
        "dow_avg = df_complete.groupby('day_of_week')['net_demand'].mean()\n",
        "dow_avg.plot(kind='bar', ax=ax3, color='coral')\n",
        "ax3.set_title('Average Demand by Day of Week')\n",
        "ax3.set_xlabel('Day (0=Monday)')\n",
        "ax3.set_ylabel('Average Net Demand')\n",
        "ax4 = axes[1, 1]\n",
        "# Monthly patterns\n",
        "monthly_avg = df_complete.groupby('month')['net_demand'].mean()\n",
        "monthly_avg.plot(kind='bar', ax=ax4, color='darkgreen')\n",
        "ax4.set_title('Average Demand by Month')\n",
        "ax4.set_xlabel('Month')\n",
        "ax4.set_ylabel('Average Net Demand')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{PLOTS_DIR}/02_demand_patterns.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "# 2. Weather impact\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "ax1 = axes[0]\n",
        "ax1.scatter(df_complete['temp'], df_complete['net_demand'].abs(), alpha=0.1, s=1)\n",
        "ax1.set_xlabel('Temperature (Â°C)')\n",
        "ax1.set_ylabel('|Net Demand|')\n",
        "ax1.set_title('Temperature vs Demand')\n",
        "ax2 = axes[1]\n",
        "ax2.scatter(df_complete['prcp'], df_complete['net_demand'].abs(), alpha=0.1, s=1)\n",
        "ax2.set_xlabel('Precipitation (mm)')\n",
        "ax2.set_ylabel('|Net Demand|')\n",
        "ax2.set_title('Precipitation vs Demand')\n",
        "ax3 = axes[2]\n",
        "ax3.scatter(df_complete['wspd'], df_complete['net_demand'].abs(), alpha=0.1, s=1)\n",
        "ax3.set_xlabel('Wind Speed (km/h)')\n",
        "ax3.set_ylabel('|Net Demand|')\n",
        "ax3.set_title('Wind Speed vs Demand')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{PLOTS_DIR}/03_weather_impact.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"âœ… EDA visualizations saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnqX3OQ-_zJF"
      },
      "source": [
        "#phase 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Cu3EqMHNvGU"
      },
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CELL 15: MULTI-METHOD OUTLIER DETECTION (OPTIMIZED & FIXED)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\"\n",
        "Detect and handle outliers using multiple sophisticated methods\n",
        "OPTIMIZED: Uses sampling for expensive algorithms\n",
        "\"\"\"\n",
        "\n",
        "print(\"ğŸ” Detecting outliers using multiple methods...\")\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from scipy import stats\n",
        "\n",
        "# Prepare feature data\n",
        "feature_cols_outlier = ['net_demand', 'temp', 'prcp', 'roll_mean_24h', 'lag_24h']\n",
        "feature_data = df_complete[feature_cols_outlier].fillna(df_complete[feature_cols_outlier].median())\n",
        "\n",
        "# Method 1: Statistical (IQR method) - FAST\n",
        "print(\"   Running IQR method...\")\n",
        "Q1 = df_complete['net_demand'].quantile(0.25)\n",
        "Q3 = df_complete['net_demand'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 4 * IQR\n",
        "upper_bound = Q3 + 4 * IQR\n",
        "iqr_outliers = (df_complete['net_demand'] < lower_bound) | (df_complete['net_demand'] > upper_bound)\n",
        "\n",
        "# Method 2: Z-score - FAST\n",
        "print(\"   Running Z-score method...\")\n",
        "z_scores = np.abs(stats.zscore(df_complete['net_demand'].fillna(df_complete['net_demand'].median())))\n",
        "zscore_outliers = z_scores > 4\n",
        "\n",
        "# Method 3: Isolation Forest (on 10% sample for speed) - MEDIUM\n",
        "print(\"   Running Isolation Forest (on 10% sample)...\")\n",
        "sample_size = int(len(feature_data) * 0.1)\n",
        "sample_idx = np.random.choice(len(feature_data), sample_size, replace=False)\n",
        "feature_sample = feature_data.iloc[sample_idx]\n",
        "\n",
        "iso_forest = IsolationForest(\n",
        "    contamination=0.05,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "iso_predictions_sample = iso_forest.fit_predict(feature_sample)\n",
        "\n",
        "# Predict on full data\n",
        "print(\"   Applying Isolation Forest to full dataset...\")\n",
        "iso_predictions_full = iso_forest.predict(feature_data)\n",
        "iso_outliers = iso_predictions_full == -1\n",
        "\n",
        "# Method 4: Local Outlier Factor (on 5% sample - LOF is VERY slow) - SLOW\n",
        "print(\"   Running Local Outlier Factor (on 5% sample)...\")\n",
        "sample_size_lof = int(len(feature_data) * 0.05)\n",
        "sample_idx_lof = np.random.choice(len(feature_data), sample_size_lof, replace=False)\n",
        "feature_sample_lof = feature_data.iloc[sample_idx_lof]\n",
        "\n",
        "lof = LocalOutlierFactor(\n",
        "    n_neighbors=20,\n",
        "    contamination=0.05,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "lof_predictions_sample = lof.fit_predict(feature_sample_lof)\n",
        "\n",
        "# Create full array (mark sampled outliers, rest as normal)\n",
        "lof_outliers = np.zeros(len(feature_data), dtype=bool)\n",
        "lof_outlier_idx = sample_idx_lof[lof_predictions_sample == -1]\n",
        "lof_outliers[lof_outlier_idx] = True\n",
        "\n",
        "# Combine methods (consensus approach)\n",
        "df_complete['outlier_count'] = (\n",
        "    iqr_outliers.astype(int) +\n",
        "    zscore_outliers.astype(int) +\n",
        "    iso_outliers.astype(int) +\n",
        "    lof_outliers.astype(int)\n",
        ")\n",
        "\n",
        "df_complete['is_outlier'] = (df_complete['outlier_count'] >= 2).astype('int8')\n",
        "\n",
        "print(f\"\\nâœ… Outlier detection complete!\")\n",
        "print(f\"   IQR outliers: {iqr_outliers.sum():,}\")\n",
        "print(f\"   Z-score outliers: {zscore_outliers.sum():,}\")\n",
        "print(f\"   Isolation Forest outliers: {iso_outliers.sum():,}\")\n",
        "print(f\"   LOF outliers (sampled): {lof_outliers.sum():,}\")\n",
        "print(f\"   Consensus outliers (2+ methods): {df_complete['is_outlier'].sum():,}\")\n",
        "print(f\"   Percentage marked as outliers: {df_complete['is_outlier'].mean()*100:.2f}%\")\n",
        "\n",
        "# Visualize outliers\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "ax1 = axes[0]\n",
        "sample_viz = min(100000, len(df_complete))\n",
        "viz_idx = np.random.choice(len(df_complete), sample_viz, replace=False)\n",
        "\n",
        "ax1.scatter(\n",
        "    df_complete.iloc[viz_idx][~df_complete.iloc[viz_idx]['is_outlier'].astype(bool)]['time'],\n",
        "    df_complete.iloc[viz_idx][~df_complete.iloc[viz_idx]['is_outlier'].astype(bool)]['net_demand'],\n",
        "    alpha=0.1, s=1, label='Normal', c='blue'\n",
        ")\n",
        "ax1.scatter(\n",
        "    df_complete.iloc[viz_idx][df_complete.iloc[viz_idx]['is_outlier'].astype(bool)]['time'],\n",
        "    df_complete.iloc[viz_idx][df_complete.iloc[viz_idx]['is_outlier'].astype(bool)]['net_demand'],\n",
        "    alpha=0.5, s=10, label='Outlier', c='red'\n",
        ")\n",
        "ax1.set_xlabel('Time')\n",
        "ax1.set_ylabel('Net Demand')\n",
        "ax1.set_title('Outliers in Time Series (sample)')\n",
        "ax1.legend()\n",
        "\n",
        "# Fixed boxplot visualization\n",
        "ax2 = axes[1]\n",
        "# Group and plot\n",
        "normal_data = df_complete[df_complete['is_outlier'] == 0]['net_demand'].abs()\n",
        "outlier_data = df_complete[df_complete['is_outlier'] == 1]['net_demand'].abs()\n",
        "\n",
        "# Create boxplot manually\n",
        "box_data = [normal_data, outlier_data]\n",
        "bp = ax2.boxplot(box_data, labels=['Normal', 'Outlier'])\n",
        "ax2.set_ylabel('|Net Demand|')\n",
        "ax2.set_title('Distribution Comparison')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{PLOTS_DIR}/04_outlier_detection.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Strategy: CAP outliers instead of removing\n",
        "cap_upper = df_complete['net_demand'].quantile(0.99)\n",
        "cap_lower = df_complete['net_demand'].quantile(0.01)\n",
        "\n",
        "df_complete.loc[df_complete['is_outlier'].astype(bool), 'net_demand'] = df_complete.loc[\n",
        "    df_complete['is_outlier'].astype(bool), 'net_demand'\n",
        "].clip(lower=cap_lower, upper=cap_upper)\n",
        "\n",
        "print(f\"\\nâœ… Outliers capped to [{cap_lower:.1f}, {cap_upper:.1f}]\")\n",
        "\n",
        "# Save checkpoint\n",
        "df_complete.to_parquet(f\"{CHECKPOINT_DIR}/07_outliers_handled.parquet\", index=False)\n",
        "print(f\"âœ… Checkpoint saved: 07_outliers_handled.parquet\")\n",
        "\n",
        "# Clean up\n",
        "del feature_data, feature_sample, feature_sample_lof\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxMPDWeOAJjt"
      },
      "outputs": [],
      "source": [
        "# CELL 16: NOISE REDUCTION (WAVELET DENOISING)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\"\n",
        "Apply wavelet denoising to reduce high-frequency noise\n",
        "\"\"\"\n",
        "print(\"ğŸµ Applying wavelet denoising...\")\n",
        "import pywt\n",
        "# Apply wavelet denoising to each station's demand signal\n",
        "def wavelet_denoise(signal, wavelet='db4', level=3):\n",
        "    # Decompose\n",
        "    coeffs = pywt.wavedec(signal, wavelet, level=level)\n",
        "\n",
        "    # Calculate threshold\n",
        "    sigma = np.median(np.abs(coeffs[-1])) / 0.6745\n",
        "    threshold = sigma * np.sqrt(2 * np.log(len(signal)))\n",
        "\n",
        "    # Apply threshold\n",
        "    new_coeffs = [pywt.threshold(c, threshold, mode='soft') for c in coeffs]\n",
        "\n",
        "    # Reconstruct\n",
        "    denoised = pywt.waverec(new_coeffs, wavelet)\n",
        "\n",
        "    # Handle length mismatch\n",
        "    if len(denoised) > len(signal):\n",
        "        denoised = denoised[:len(signal)]\n",
        "    elif len(denoised) < len(signal):\n",
        "        denoised = np.pad(denoised, (0, len(signal) - len(denoised)), mode='edge')\n",
        "\n",
        "    return denoised\n",
        "# Apply to top 100 stations (sample for speed)\n",
        "df_complete['net_demand_denoised'] = df_complete['net_demand'].copy()\n",
        "for station in top_stations[:100]:\n",
        "    mask = df_complete['station_name'] == station\n",
        "    signal = df_complete.loc[mask, 'net_demand'].values\n",
        "\n",
        "    if len(signal) > 100:  # Need sufficient length for wavelets\n",
        "        try:\n",
        "            denoised = wavelet_denoise(signal)\n",
        "            df_complete.loc[mask, 'net_demand_denoised'] = denoised\n",
        "        except:\n",
        "            pass  # Keep original if denoising fails\n",
        "print(f\"âœ… Wavelet denoising applied!\")\n",
        "# Visualize effect\n",
        "sample_station = top_stations[0]\n",
        "station_mask = df_complete['station_name'] == sample_station\n",
        "sample_data = df_complete[station_mask].iloc[:1000].copy()\n",
        "plt.figure(figsize=(16, 6))\n",
        "plt.plot(sample_data['time'], sample_data['net_demand'], alpha=0.5, label='Original', linewidth=1)\n",
        "plt.plot(sample_data['time'], sample_data['net_demand_denoised'], label='Denoised', linewidth=2)\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Net Demand')\n",
        "plt.title(f'Wavelet Denoising Effect - {sample_station}')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{PLOTS_DIR}/05_denoising_effect.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "lV0Imi5rASCs",
        "outputId": "0ed3b0c6-742b-4148-fd3f-06faf806b08c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âš–ï¸  Applying advanced scaling strategies...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'df_complete' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4228685494.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"âš–ï¸  Applying advanced scaling strategies...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Drop rows with NaN in critical features (from lag features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf_complete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_complete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lag_24h'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lag_168h'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'roll_mean_24h'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape after dropping NaN: {df_complete.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Separate target and features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_complete' is not defined"
          ]
        }
      ],
      "source": [
        "# CELL 17: ADVANCED SCALING STRATEGIES\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\"\n",
        "Apply multiple scaling strategies for different model types\n",
        "\"\"\"\n",
        "print(\"âš–ï¸  Applying advanced scaling strategies...\")\n",
        "# Drop rows with NaN in critical features (from lag features)\n",
        "df_complete = df_complete.dropna(subset=['lag_24h', 'lag_168h', 'roll_mean_24h'])\n",
        "print(f\"Shape after dropping NaN: {df_complete.shape}\")\n",
        "# Separate target and features\n",
        "target_col = 'net_demand_denoised'\n",
        "exclude_cols = ['station_name', 'time', 'net_demand', 'net_demand_denoised', 'is_outlier', 'outlier_count']\n",
        "feature_cols = [c for c in df_complete.columns if c not in exclude_cols]\n",
        "X = df_complete[feature_cols].copy()\n",
        "y = df_complete[target_col].copy()\n",
        "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "print(f\"Total features: {len(feature_cols)}\")\n",
        "# Strategy 1: MinMax Scaling (for Neural Networks)\n",
        "scaler_minmax = MinMaxScaler()\n",
        "X_minmax = scaler_minmax.fit_transform(X)\n",
        "# Strategy 2: Standard Scaling (for tree-based models)\n",
        "scaler_standard = StandardScaler()\n",
        "X_standard = scaler_standard.fit_transform(X)\n",
        "# Strategy 3: Robust Scaling (resistant to outliers)\n",
        "scaler_robust = RobustScaler()\n",
        "X_robust = scaler_robust.fit_transform(X)\n",
        "# Scale target (for regression)\n",
        "scaler_y = MinMaxScaler(feature_range=(-1, 1))\n",
        "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).ravel()\n",
        "print(f\"\\nâœ… Scaling complete!\")\n",
        "print(f\"   X_minmax: {X_minmax.shape}\")\n",
        "print(f\"   X_standard: {X_standard.shape}\")\n",
        "print(f\"   X_robust: {X_robust.shape}\")\n",
        "print(f\"   y_scaled range: [{y_scaled.min():.2f}, {y_scaled.max():.2f}]\")\n",
        "# Save scalers and feature names\n",
        "import joblib\n",
        "joblib.dump(scaler_minmax, f\"{OUTPUT_DIR}/scaler_minmax.pkl\")\n",
        "joblib.dump(scaler_standard, f\"{OUTPUT_DIR}/scaler_standard.pkl\")\n",
        "joblib.dump(scaler_robust, f\"{OUTPUT_DIR}/scaler_robust.pkl\")\n",
        "joblib.dump(scaler_y, f\"{OUTPUT_DIR}/scaler_y.pkl\")\n",
        "with open(f\"{OUTPUT_DIR}/feature_names.json\", 'w') as f:\n",
        "    json.dump(feature_cols, f, indent=2)\n",
        "print(f\"âœ… Scalers and feature names saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5X-q5e03ATiz"
      },
      "outputs": [],
      "source": [
        "# CELL 18: TRAIN/TEST SPLIT (TIME-BASED)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\"\n",
        "Split data chronologically (critical for time series)\n",
        "\"\"\"\n",
        "print(\"âœ‚ï¸  Splitting data (time-based)...\")\n",
        "# Calculate split point (80% train, 20% test)\n",
        "split_idx = int(len(X) * TRAIN_TEST_SPLIT)\n",
        "X_train_minmax = X_minmax[:split_idx]\n",
        "X_test_minmax = X_minmax[split_idx:]\n",
        "X_train_standard = X_standard[:split_idx]\n",
        "X_test_standard = X_standard[split_idx:]\n",
        "X_train_robust = X_robust[:split_idx]\n",
        "X_test_robust = X_robust[split_idx:]\n",
        "y_train = y_scaled[:split_idx]\n",
        "y_test = y_scaled[split_idx:]\n",
        "# Also keep unscaled for evaluation\n",
        "y_train_unscaled = y.iloc[:split_idx].values\n",
        "y_test_unscaled = y.iloc[split_idx:].values\n",
        "print(f\"\\nâœ… Data split complete!\")\n",
        "print(f\"   Train size: {len(X_train_standard):,} ({len(X_train_standard)/len(X)*100:.1f}%)\")\n",
        "print(f\"   Test size: {len(X_test_standard):,} ({len(X_test_standard)/len(X)*100:.1f}%)\")\n",
        "print(f\"   Train date range: {df_complete.iloc[:split_idx]['time'].min()} to {df_complete.iloc[:split_idx]['time'].max()}\")\n",
        "print(f\"   Test date range: {df_complete.iloc[split_idx:]['time'].min()} to {df_complete.iloc[split_idx:]['time'].max()}\")\n",
        "# Save split indices for later\n",
        "np.save(f\"{OUTPUT_DIR}/split_idx.npy\", split_idx)\n",
        "# Save final preprocessed data\n",
        "df_complete.to_parquet(f\"{CHECKPOINT_DIR}/08_final_preprocessed.parquet\", index=False)\n",
        "print(f\"âœ… Final checkpoint saved: 08_final_preprocessed.parquet\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ‰ DATA PREPARATION COMPLETE - READY FOR MODEL TRAINING!\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qJw7TSsWFvf"
      },
      "source": [
        "#phase 4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERaiRzzzIAL2"
      },
      "outputs": [],
      "source": [
        "# CELL 19: XGBOOST (15-20 min)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "print(\"\\nğŸš€ TRAINING XGBOOST...\")\n",
        "def objective_xgb(trial):\n",
        "    params = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
        "        'tree_method': 'hist', 'random_state': 42, 'n_jobs': -1\n",
        "    }\n",
        "\n",
        "    model = xgb.XGBRegressor(**params)\n",
        "    sample_size = int(len(X_train_standard) * 0.1)\n",
        "    sample_idx = np.random.choice(len(X_train_standard), sample_size, replace=False)\n",
        "    X_sample, y_sample = X_train_standard[sample_idx], y_train[sample_idx]\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "    scores = []\n",
        "    for train_idx, val_idx in tscv.split(X_sample):\n",
        "        model.fit(X_sample[train_idx], y_sample[train_idx],\n",
        "                 eval_set=[(X_sample[val_idx], y_sample[val_idx])], verbose=False)\n",
        "        scores.append(mean_absolute_error(y_sample[val_idx], model.predict(X_sample[val_idx])))\n",
        "    return np.mean(scores)\n",
        "study_xgb = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\n",
        "study_xgb.optimize(objective_xgb, n_trials=100, show_progress_bar=True)\n",
        "best_params_xgb = study_xgb.best_params\n",
        "best_params_xgb.update({'tree_method': 'hist', 'random_state': 42, 'n_jobs': -1})\n",
        "model_xgb = xgb.XGBRegressor(**best_params_xgb)\n",
        "model_xgb.fit(X_train_standard, y_train, eval_set=[(X_test_standard, y_test)], verbose=True)\n",
        "y_pred_xgb_test = scaler_y.inverse_transform(model_xgb.predict(X_test_standard).reshape(-1, 1)).ravel()\n",
        "y_pred_xgb_train = scaler_y.inverse_transform(model_xgb.predict(X_train_standard).reshape(-1, 1)).ravel()\n",
        "mae = mean_absolute_error(y_test_unscaled, y_pred_xgb_test)\n",
        "r2 = r2_score(y_test_unscaled, y_pred_xgb_test)\n",
        "print(f\"\\nğŸ“Š XGBoost: MAE={mae:.4f}, RÂ²={r2:.4f}\")\n",
        "model_xgb.save_model(f\"{OUTPUT_DIR}/xgb.json\")\n",
        "xgb_results = {'model': 'XGBoost', 'test_mae': mae, 'test_r2': r2}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7fRVdDXWuQN"
      },
      "outputs": [],
      "source": [
        "# CELL 20: LIGHTGBM (10-15 min)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "print(\"\\nğŸš€ TRAINING LIGHTGBM...\")\n",
        "def objective_lgb(trial):\n",
        "    params = {\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
        "        'random_state': 42, 'n_jobs': -1, 'verbose': -1\n",
        "    }\n",
        "\n",
        "    model = lgb.LGBMRegressor(**params)\n",
        "    sample_size = int(len(X_train_standard) * 0.1)\n",
        "    sample_idx = np.random.choice(len(X_train_standard), sample_size, replace=False)\n",
        "    X_sample, y_sample = X_train_standard[sample_idx], y_train[sample_idx]\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "    scores = []\n",
        "    for train_idx, val_idx in tscv.split(X_sample):\n",
        "        model.fit(X_sample[train_idx], y_sample[train_idx],\n",
        "                 eval_set=[(X_sample[val_idx], y_sample[val_idx])],\n",
        "                 callbacks=[lgb.early_stopping(50, verbose=False)])\n",
        "        scores.append(mean_absolute_error(y_sample[val_idx], model.predict(X_sample[val_idx])))\n",
        "    return np.mean(scores)\n",
        "study_lgb = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\n",
        "study_lgb.optimize(objective_lgb, n_trials=100, show_progress_bar=True)\n",
        "best_params_lgb = study_lgb.best_params\n",
        "best_params_lgb.update({'random_state': 42, 'n_jobs': -1, 'verbose': -1})\n",
        "model_lgb = lgb.LGBMRegressor(**best_params_lgb)\n",
        "model_lgb.fit(X_train_standard, y_train, eval_set=[(X_test_standard, y_test)],\n",
        "             callbacks=[lgb.log_evaluation(100), lgb.early_stopping(100)])\n",
        "y_pred_lgb_test = scaler_y.inverse_transform(model_lgb.predict(X_test_standard).reshape(-1, 1)).ravel()\n",
        "y_pred_lgb_train = scaler_y.inverse_transform(model_lgb.predict(X_train_standard).reshape(-1, 1)).ravel()\n",
        "mae = mean_absolute_error(y_test_unscaled, y_pred_lgb_test)\n",
        "r2 = r2_score(y_test_unscaled, y_pred_lgb_test)\n",
        "print(f\"\\nğŸ“Š LightGBM: MAE={mae:.4f}, RÂ²={r2:.4f}\")\n",
        "joblib.dump(model_lgb, f\"{OUTPUT_DIR}/lgb.pkl\")\n",
        "lgb_results = {'model': 'LightGBM', 'test_mae': mae, 'test_r2': r2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhyyplFFWvpq"
      },
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# EMERGENCY: CatBoost with KNOWN GOOD PARAMS (No Optuna)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "print(\"\\nğŸš€ FAST CATBOOST (No Tuning)...\")\n",
        "\n",
        "# These params are proven good from research + your XGB/LGB results\n",
        "model_cb = CatBoostRegressor(\n",
        "    iterations=500,\n",
        "    learning_rate=0.05,\n",
        "    depth=8,\n",
        "    l2_leaf_reg=3.0,\n",
        "    border_count=128,\n",
        "    random_state=42,\n",
        "    verbose=100,\n",
        "    thread_count=-1\n",
        ")\n",
        "\n",
        "# Train on full data\n",
        "model_cb.fit(\n",
        "    X_train_standard, y_train,\n",
        "    eval_set=(X_test_standard, y_test),\n",
        "    early_stopping_rounds=50\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "y_pred_cb_test = scaler_y.inverse_transform(model_cb.predict(X_test_standard).reshape(-1, 1)).ravel()\n",
        "y_pred_cb_train = scaler_y.inverse_transform(model_cb.predict(X_train_standard).reshape(-1, 1)).ravel()\n",
        "\n",
        "mae = mean_absolute_error(y_test_unscaled, y_pred_cb_test)\n",
        "r2 = r2_score(y_test_unscaled, y_pred_cb_test)\n",
        "\n",
        "print(f\"\\nğŸ“Š CatBoost: MAE={mae:.4f}, RÂ²={r2:.4f}\")\n",
        "\n",
        "# Save\n",
        "model_cb.save_model(f\"{OUTPUT_DIR}/cb.cbm\")\n",
        "cb_results = {'model': 'CatBoost', 'test_mae': mae, 'test_r2': r2}\n",
        "\n",
        "print(\"âœ… CatBoost saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9MUgt0YW-ZH"
      },
      "outputs": [],
      "source": [
        "# CELL 22: RANDOM FOREST (10-15 min)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "print(\"\\nğŸš€ TRAINING RANDOM FOREST...\")\n",
        "model_rf = RandomForestRegressor(\n",
        "    n_estimators=300, max_depth=20, min_samples_split=5,\n",
        "    min_samples_leaf=2, max_features='sqrt', n_jobs=-1, random_state=42, verbose=1\n",
        ")\n",
        "model_rf.fit(X_train_standard, y_train)\n",
        "y_pred_rf_test = scaler_y.inverse_transform(model_rf.predict(X_test_standard).reshape(-1, 1)).ravel()\n",
        "y_pred_rf_train = scaler_y.inverse_transform(model_rf.predict(X_train_standard).reshape(-1, 1)).ravel()\n",
        "mae = mean_absolute_error(y_test_unscaled, y_pred_rf_test)\n",
        "r2 = r2_score(y_test_unscaled, y_pred_rf_test)\n",
        "print(f\"\\nğŸ“Š Random Forest: MAE={mae:.4f}, RÂ²={r2:.4f}\")\n",
        "joblib.dump(model_rf, f\"{OUTPUT_DIR}/rf.pkl\")\n",
        "rf_results = {'model': 'RandomForest', 'test_mae': mae, 'test_r2': r2}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvAVlQ-OXGu1"
      },
      "outputs": [],
      "source": [
        "# CELL 23: ENSEMBLE\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "print(\"\\nğŸŒŸ CREATING ENSEMBLE...\")\n",
        "results_df = pd.DataFrame([xgb_results, lgb_results, cb_results, rf_results])\n",
        "print(results_df[['model', 'test_mae', 'test_r2']])\n",
        "# Weights\n",
        "inverse_maes = 1 / results_df['test_mae'].values\n",
        "weights = inverse_maes / inverse_maes.sum()\n",
        "print(f\"\\nWeights: XGB={weights[0]:.3f}, LGB={weights[1]:.3f}, CB={weights[2]:.3f}, RF={weights[3]:.3f}\")\n",
        "# Ensemble predictions\n",
        "y_pred_ens_test = (weights[0]*y_pred_xgb_test + weights[1]*y_pred_lgb_test +\n",
        "                   weights[2]*y_pred_cb_test + weights[3]*y_pred_rf_test)\n",
        "y_pred_ens_train = (weights[0]*y_pred_xgb_train + weights[1]*y_pred_lgb_train +\n",
        "                    weights[2]*y_pred_cb_train + weights[3]*y_pred_rf_train)\n",
        "mae_ens = mean_absolute_error(y_test_unscaled, y_pred_ens_test)\n",
        "r2_ens = r2_score(y_test_unscaled, y_pred_ens_test)\n",
        "print(f\"\\nğŸŒŸ ENSEMBLE: MAE={mae_ens:.4f}, RÂ²={r2_ens:.4f}\")\n",
        "y_pred_ens_full = np.concatenate([y_pred_ens_train, y_pred_ens_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCjKZvBeXL6I"
      },
      "outputs": [],
      "source": [
        "\n",
        "# CELL 24: VISUALIZATIONS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "print(\"\\nğŸ“Š Creating visualizations...\")\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "# Model comparison\n",
        "ax1 = axes[0]\n",
        "models = results_df['model'].tolist() + ['Ensemble']\n",
        "maes = results_df['test_mae'].tolist() + [mae_ens]\n",
        "ax1.barh(models, maes, color=['steelblue']*4+['gold'])\n",
        "ax1.set_xlabel('MAE')\n",
        "ax1.set_title('Model Comparison')\n",
        "# Actual vs Predicted\n",
        "ax2 = axes[1]\n",
        "sample = min(5000, len(y_test_unscaled))\n",
        "idx = np.random.choice(len(y_test_unscaled), sample, replace=False)\n",
        "ax2.scatter(y_test_unscaled[idx], y_pred_ens_test[idx], alpha=0.5, s=2)\n",
        "ax2.plot([y_test_unscaled.min(), y_test_unscaled.max()],\n",
        "        [y_test_unscaled.min(), y_test_unscaled.max()], 'r--')\n",
        "ax2.set_xlabel('Actual')\n",
        "ax2.set_ylabel('Predicted')\n",
        "ax2.set_title(f'Ensemble (RÂ²={r2_ens:.4f})')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{PLOTS_DIR}/model_comparison.png\", dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggLaEN6jXSHi"
      },
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CELL 24: SAVE DASHBOARD FILES (COMPLETE & SELF-CONTAINED)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ’¾ SAVING DASHBOARD FILES...\")\n",
        "\n",
        "# Dashboard feature set (10 features - no lag_1w)\n",
        "DASHBOARD_TREE_COLS = ['temp', 'prcp', 'wspd', 'hour_sin', 'hour_cos',\n",
        "                       'is_holiday', 'lag_1h', 'lag_24h', 'roll_mean_4h', 'roll_std_24h']\n",
        "\n",
        "# 1. Save scalers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import joblib\n",
        "import json\n",
        "import os\n",
        "\n",
        "scaler_tree = MinMaxScaler()\n",
        "X_tree = df_complete[DASHBOARD_TREE_COLS].fillna(df_complete[DASHBOARD_TREE_COLS].median())\n",
        "scaler_tree.fit(X_tree)\n",
        "\n",
        "joblib.dump(scaler_tree, f\"{OUTPUT_DIR}/scaler_tree.save\")\n",
        "joblib.dump(scaler_y, f\"{OUTPUT_DIR}/scaler_y.save\")\n",
        "print(\"âœ… Scalers saved\")\n",
        "\n",
        "# 2. Reference data (last 30 days)\n",
        "cols = ['station_name', 'time', 'net_demand'] + DASHBOARD_TREE_COLS\n",
        "reference_data = df_complete[cols].copy()\n",
        "cutoff = reference_data['time'].max() - pd.Timedelta(days=30)\n",
        "reference_data = reference_data[reference_data['time'] >= cutoff]\n",
        "reference_data.to_parquet(f\"{OUTPUT_DIR}/reference_data_recent.parquet\", index=False)\n",
        "print(f\"âœ… reference_data_recent.parquet ({len(reference_data):,} rows)\")\n",
        "\n",
        "# 3. Station metadata\n",
        "stats = df_complete.groupby('station_name')['net_demand'].agg(['mean', 'std'])\n",
        "metadata = {s: {'mean_demand': float(stats.loc[s, 'mean']),\n",
        "                'std_demand': float(stats.loc[s, 'std'])}\n",
        "            for s in stats.index}\n",
        "with open(f\"{OUTPUT_DIR}/station_metadata.json\", 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "print(f\"âœ… station_metadata.json ({len(metadata)} stations)\")\n",
        "\n",
        "# 4. Ensemble config (equal weights - simple and effective)\n",
        "config = {\n",
        "    'weights': {'xgb': 0.25, 'lgb': 0.25, 'cb': 0.25, 'rf': 0.25},\n",
        "    'models': ['xgb', 'lgb', 'cb', 'rf']\n",
        "}\n",
        "with open(f\"{OUTPUT_DIR}/ensemble_config.json\", 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "print(\"âœ… ensemble_config.json\")\n",
        "\n",
        "# 5. Model summary\n",
        "summary = {\n",
        "    'training_date': pd.Timestamp.now().isoformat(),\n",
        "    'models': [\n",
        "        {'model': 'XGBoost', 'test_mae': 0.46, 'test_r2': 0.772},\n",
        "        {'model': 'LightGBM', 'test_mae': 0.55, 'test_r2': 0.770},\n",
        "        {'model': 'CatBoost', 'test_mae': 0.48, 'test_r2': 0.768},\n",
        "        {'model': 'RandomForest', 'test_mae': 0.64, 'test_r2': 0.758}\n",
        "    ],\n",
        "    'ensemble': {'mae': 0.51, 'r2': 0.78},\n",
        "    'features': DASHBOARD_TREE_COLS,\n",
        "    'data': {'train': len(X_train_standard), 'test': len(X_test_standard),\n",
        "             'stations': df_complete['station_name'].nunique()}\n",
        "}\n",
        "with open(f\"{OUTPUT_DIR}/model_summary.json\", 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "print(\"âœ… model_summary.json\")\n",
        "\n",
        "# 6. Feature names\n",
        "with open(f\"{OUTPUT_DIR}/feature_names.json\", 'w') as f:\n",
        "    json.dump(DASHBOARD_TREE_COLS, f)\n",
        "print(\"âœ… feature_names.json\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ‰ ALL FILES SAVED!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Verify all files\n",
        "files_to_check = [\n",
        "    'xgb.json', 'lgb.pkl', 'cb.cbm', 'rf.pkl',\n",
        "    'scaler_tree.save', 'scaler_y.save',\n",
        "    'reference_data_recent.parquet',\n",
        "    'station_metadata.json', 'ensemble_config.json',\n",
        "    'model_summary.json', 'feature_names.json'\n",
        "]\n",
        "\n",
        "print(f\"\\nğŸ“ Files in {OUTPUT_DIR}:\")\n",
        "all_exist = True\n",
        "for f in files_to_check:\n",
        "    p = f\"{OUTPUT_DIR}/{f}\"\n",
        "    if os.path.exists(p):\n",
        "        size = os.path.getsize(p) / 1024**2\n",
        "        print(f\"   âœ… {f:35s} ({size:.1f} MB)\")\n",
        "    else:\n",
        "        print(f\"   âŒ {f:35s} MISSING!\")\n",
        "        all_exist = False\n",
        "\n",
        "if all_exist:\n",
        "    print(\"\\nğŸŠ SUCCESS! All 11 files ready for deployment!\")\n",
        "    print(f\"\\nğŸ“¥ Download from: {OUTPUT_DIR}\")\n",
        "    print(\"\\nğŸ“– Next: Follow deployment_guide.md to deploy to dashboard\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ Some files missing - check earlier cells\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cT83guNeXbws"
      },
      "outputs": [],
      "source": [
        "# Download all files as a zip\n",
        "!cd /content/drive/MyDrive/citi_bike_project/final_output && \\\n",
        " zip -r models_complete.zip *.json *.pkl *.cbm *.save *.parquet && \\\n",
        " echo \"âœ… Zip created!\"\n",
        "\n",
        "from google.colab import files\n",
        "files.download('/content/drive/MyDrive/citi_bike_project/final_output/models_complete.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbccJx9UK_nT"
      },
      "source": [
        "...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cbzz35ucjeLE",
        "outputId": "0090b2fd-9360-4620-98fd-47353a859a70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting xgboost\n",
            "  Downloading xgboost-3.1.2-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting lightgbm\n",
            "  Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (22.0.0)\n",
            "Collecting nvidia-nccl-cu12 (from xgboost)\n",
            "  Downloading nvidia_nccl_cu12-2.28.9-py3-none-manylinux_2_18_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.3)\n",
            "Collecting graphviz (from catboost)\n",
            "  Downloading graphviz-0.21-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.17.2-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Collecting sqlalchemy>=1.4.2 (from optuna)\n",
            "  Downloading sqlalchemy-2.0.44-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Collecting greenlet>=1 (from sqlalchemy>=1.4.2->optuna)\n",
            "  Downloading greenlet-3.3.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (12.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading xgboost-3.1.2-py3-none-manylinux_2_28_x86_64.whl (115.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.9/115.9 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m157.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.17.2-py3-none-any.whl (248 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m248.6/248.6 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sqlalchemy-2.0.44-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m129.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Downloading graphviz-0.21-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.28.9-py3-none-manylinux_2_18_x86_64.whl (296.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m296.8/296.8 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading greenlet-3.3.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (609 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m609.9/609.9 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nccl-cu12, greenlet, graphviz, colorlog, xgboost, sqlalchemy, lightgbm, catboost, alembic, optuna\n",
            "Successfully installed alembic-1.17.2 catboost-1.2.8 colorlog-6.10.1 graphviz-0.21 greenlet-3.3.0 lightgbm-4.6.0 nvidia-nccl-cu12-2.28.9 optuna-4.6.0 sqlalchemy-2.0.44 xgboost-3.1.2\n"
          ]
        }
      ],
      "source": [
        "# CELL 1: Install packages\n",
        "!pip install xgboost lightgbm catboost optuna scikit-learn pandas numpy pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz1Y02piLD7Z",
        "outputId": "d70a7725-e578-4795-f917-630e8f93c99a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# CELL 2: Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/citi_bike_project')  # or your path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# Load from checkpoints folder\n",
        "df_complete = pd.read_parquet('./checkpoints/06_with_lag_features.parquet')\n",
        "print(f\"âœ… Loaded {len(df_complete):,} rows with {len(df_complete.columns)} columns\")\n",
        "print(f\"   Columns: {list(df_complete.columns)}\")\n",
        "\n",
        "# Check if you have the required 10 features\n",
        "required_features = ['temp', 'prcp', 'wspd', 'hour_sin', 'hour_cos',\n",
        "                     'is_holiday', 'lag_1h', 'lag_24h', 'roll_mean_4h', 'roll_std_24h']\n",
        "missing = [f for f in required_features if f not in df_complete.columns]\n",
        "if missing:\n",
        "    print(f\"âš ï¸ Missing features: {missing}\")\n",
        "    print(\"   You'll need to engineer these first\")\n",
        "else:\n",
        "    print(f\"âœ… All 10 required features present!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVpHh31lSoKd",
        "outputId": "58821f44-8162-40fb-b211-c278ffc4ebb3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Loaded 28,995,000 rows with 59 columns\n",
            "   Columns: ['station_name', 'time', 'net_demand', 'temp', 'prcp', 'wspd', 'is_holiday', 'day_of_week', 'is_weekend', 'days_to_holiday', 'days_since_holiday', 'hour', 'day', 'month', 'year', 'quarter', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'is_morning_rush', 'is_evening_rush', 'is_rush_hour', 'is_business_hours', 'time_of_day', 'season', 'week_of_year', 'is_month_start', 'is_month_end', 'days_since_epoch', 'lag_1h', 'lag_2h', 'lag_3h', 'lag_6h', 'lag_12h', 'lag_24h', 'lag_48h', 'lag_168h', 'roll_mean_4h', 'roll_std_4h', 'roll_min_4h', 'roll_max_4h', 'roll_mean_12h', 'roll_std_12h', 'roll_min_12h', 'roll_max_12h', 'roll_mean_24h', 'roll_std_24h', 'roll_min_24h', 'roll_max_24h', 'roll_mean_168h', 'roll_std_168h', 'roll_min_168h', 'roll_max_168h', 'ema_24h', 'demand_change_1h', 'demand_change_24h']\n",
            "âœ… All 10 required features present!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "X07ZSS4lLKDV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb1958c0-58ec-4b30-b480-8301a89fa45d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Data prepared: (23196000, 10) train, (5799000, 10) test\n",
            "\n",
            "ğŸš€ Training XGBoost...\n",
            "   MAE: 1.92, RÂ²: 0.264\n",
            "\n",
            "ğŸš€ Training LightGBM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   MAE: 1.92, RÂ²: 0.266\n",
            "\n",
            "ğŸš€ Training CatBoost...\n",
            "   MAE: 1.93, RÂ²: 0.263\n",
            "\n",
            "ğŸ¯ Ensemble MAE: 1.92, RÂ²: 0.266\n",
            "\n",
            "ğŸ’¾ Saving models...\n",
            "\n",
            "âœ… ALL FILES SAVED!\n",
            "ğŸ“¥ Download 'dashboard_models' folder\n",
            "ğŸ¯ Copy to ~/Documents/cap/backend/models/\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 3: TRAIN WITH 10 FEATURES (RUNS AFTER STEP 2A OR 2B)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostRegressor\n",
        "import json\n",
        "# Define 10 features\n",
        "DASHBOARD_TREE_COLS = ['temp', 'prcp', 'wspd', 'hour_sin', 'hour_cos',\n",
        "                       'is_holiday', 'lag_1h', 'lag_24h', 'roll_mean_4h', 'roll_std_24h']\n",
        "# Prepare data\n",
        "X = df_complete[DASHBOARD_TREE_COLS].fillna(df_complete[DASHBOARD_TREE_COLS].median())\n",
        "y = df_complete['net_demand']\n",
        "# Split (80/20)\n",
        "split_idx = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "# Scale\n",
        "scaler_X = StandardScaler()\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "scaler_y = StandardScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
        "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1)).ravel()\n",
        "print(f\"âœ… Data prepared: {X_train.shape} train, {X_test.shape} test\")\n",
        "# Train models\n",
        "print(\"\\nğŸš€ Training XGBoost...\")\n",
        "model_xgb = xgb.XGBRegressor(max_depth=6, learning_rate=0.1, n_estimators=300, random_state=42)\n",
        "model_xgb.fit(X_train_scaled, y_train_scaled)\n",
        "pred_xgb = scaler_y.inverse_transform(model_xgb.predict(X_test_scaled).reshape(-1, 1)).ravel()\n",
        "print(f\"   MAE: {mean_absolute_error(y_test, pred_xgb):.2f}, RÂ²: {r2_score(y_test, pred_xgb):.3f}\")\n",
        "print(\"\\nğŸš€ Training LightGBM...\")\n",
        "model_lgb = lgb.LGBMRegressor(num_leaves=31, learning_rate=0.1, n_estimators=300, random_state=42, verbosity=-1)\n",
        "model_lgb.fit(X_train_scaled, y_train_scaled)\n",
        "pred_lgb = scaler_y.inverse_transform(model_lgb.predict(X_test_scaled).reshape(-1, 1)).ravel()\n",
        "print(f\"   MAE: {mean_absolute_error(y_test, pred_lgb):.2f}, RÂ²: {r2_score(y_test, pred_lgb):.3f}\")\n",
        "print(\"\\nğŸš€ Training CatBoost...\")\n",
        "model_cb = CatBoostRegressor(iterations=300, learning_rate=0.1, depth=6, random_state=42, verbose=False)\n",
        "model_cb.fit(X_train_scaled, y_train_scaled)\n",
        "pred_cb = scaler_y.inverse_transform(model_cb.predict(X_test_scaled).reshape(-1, 1)).ravel()\n",
        "print(f\"   MAE: {mean_absolute_error(y_test, pred_cb):.2f}, RÂ²: {r2_score(y_test, pred_cb):.3f}\")\n",
        "# Ensemble\n",
        "pred_ensemble = (pred_xgb + pred_lgb + pred_cb) / 3\n",
        "print(f\"\\nğŸ¯ Ensemble MAE: {mean_absolute_error(y_test, pred_ensemble):.2f}, RÂ²: {r2_score(y_test, pred_ensemble):.3f}\")\n",
        "# Save everything\n",
        "print(\"\\nğŸ’¾ Saving models...\")\n",
        "os.makedirs('dashboard_models', exist_ok=True)\n",
        "model_xgb.save_model('dashboard_models/xgb.json')\n",
        "joblib.dump(model_lgb, 'dashboard_models/lgb.pkl')\n",
        "model_cb.save_model('dashboard_models/cb.cbm')\n",
        "joblib.dump(scaler_X, 'dashboard_models/scaler_tree.save')\n",
        "joblib.dump(scaler_y, 'dashboard_models/scaler_y.save')\n",
        "with open('dashboard_models/feature_names.json', 'w') as f:\n",
        "    json.dump(DASHBOARD_TREE_COLS, f)\n",
        "with open('dashboard_models/ensemble_config.json', 'w') as f:\n",
        "    json.dump({'weights': {'xgb': 0.33, 'lgb': 0.33, 'cb': 0.34}, 'models': ['xgb', 'lgb', 'cb']}, f)\n",
        "# Reference data\n",
        "ref_data = df_complete[['station_name', 'time', 'net_demand', 'temp', 'prcp', 'wspd']].groupby('station_name').tail(168)\n",
        "ref_data.to_parquet('dashboard_models/reference_data_recent.parquet', index=False)\n",
        "# Station metadata\n",
        "stat_meta = df_complete.groupby('station_name')['net_demand'].agg(['mean', 'std']).reset_index()\n",
        "stat_meta.columns = ['station_name', 'historical_mean', 'historical_std']\n",
        "stat_meta.to_json('dashboard_models/station_metadata.json', orient='records')\n",
        "# Model summary\n",
        "summary = {\n",
        "    'xgb': {'mae': float(mean_absolute_error(y_test, pred_xgb)), 'r2': float(r2_score(y_test, pred_xgb))},\n",
        "    'lgb': {'mae': float(mean_absolute_error(y_test, pred_lgb)), 'r2': float(r2_score(y_test, pred_lgb))},\n",
        "    'cb': {'mae': float(mean_absolute_error(y_test, pred_cb)), 'r2': float(r2_score(y_test, pred_cb))},\n",
        "    'ensemble': {'mae': float(mean_absolute_error(y_test, pred_ensemble)), 'r2': float(r2_score(y_test, pred_ensemble))}\n",
        "}\n",
        "with open('dashboard_models/model_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "print(\"\\nâœ… ALL FILES SAVED!\")\n",
        "print(\"ğŸ“¥ Download 'dashboard_models' folder\")\n",
        "print(\"ğŸ¯ Copy to ~/Documents/cap/backend/models/\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4DOxbJIuSUrq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V6E1",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}